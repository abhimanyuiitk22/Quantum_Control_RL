{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7beb1428-16bb-4009-bd90-6f6ef1761dae",
   "metadata": {},
   "source": [
    "#Sivak Model\n",
    "This code is from https://github.com/arthurostrauss/Quantum_Optimal_Control/blob/main/gate_level/yale_educational_example/yale_model_free_two_level_system_v2.py \n",
    "from Aurthor strauss attempting to run the QOMDP from this paper of Sivak, yale, V. Sivak, A. Eickbusch, H. Liu, B. Royer, I. Tsioutsios, and M. H. Devoret, “Model Free Quantum Control with Reinforcement Learning”, Physical Review X, vol. 12, no. 1, p. 011 059, Mar. 2022 one of the pioneering works in this field, complicated ideas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09e808ad-e9dd-4b69-9943-c379131250f1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "This version of TensorFlow Probability requires TensorFlow version >= 2.18; Detected an installation of version 2.10.0. Please upgrade TensorFlow to proceed.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mqiskit_aer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mqasm_simulator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m QasmSimulator\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mqiskit\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquantum_info\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mqi\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_probability\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Normal\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m norm\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py310\\lib\\site-packages\\tensorflow_probability\\__init__.py:22\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Tools for probabilistic reasoning in TensorFlow.\"\"\"\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Contributors to the `python/` dir should not alter this file; instead update\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# `python/__init__.py` as necessary.\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# from tensorflow_probability.google import staging  # DisableOnExport\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# from tensorflow_probability.google import tfp_google  # DisableOnExport\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_probability\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# pylint: disable=wildcard-import\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_probability\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# tfp_google.bind(globals())  # DisableOnExport\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# del tfp_google  # DisableOnExport\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py310\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:152\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _tf_loaded():\n\u001b[0;32m    150\u001b[0m   \u001b[38;5;66;03m# Non-lazy load of packages that register with tensorflow or keras.\u001b[39;00m\n\u001b[0;32m    151\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m pkg_name \u001b[38;5;129;01min\u001b[39;00m _maybe_nonlazy_load:\n\u001b[1;32m--> 152\u001b[0m     \u001b[38;5;28;43mdir\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mglobals\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpkg_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Forces loading the package from its lazy loader.\u001b[39;00m\n\u001b[0;32m    155\u001b[0m all_util\u001b[38;5;241m.\u001b[39mremove_undocumented(\u001b[38;5;18m__name__\u001b[39m, _lazy_load \u001b[38;5;241m+\u001b[39m _maybe_nonlazy_load)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py310\\lib\\site-packages\\tensorflow_probability\\python\\internal\\lazy_loader.py:60\u001b[0m, in \u001b[0;36mLazyLoader.__dir__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__dir__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m---> 60\u001b[0m   module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mdir\u001b[39m(module)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py310\\lib\\site-packages\\tensorflow_probability\\python\\internal\\lazy_loader.py:40\u001b[0m, in \u001b[0;36mLazyLoader._load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load the module and insert it into the parent's globals.\"\"\"\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_on_first_access):\n\u001b[1;32m---> 40\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_on_first_access\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_on_first_access \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Import the target module and insert it into the parent's namespace\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py310\\lib\\site-packages\\tensorflow_probability\\python\\__init__.py:59\u001b[0m, in \u001b[0;36m_validate_tf_environment\u001b[1;34m(package)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m#   required_tensorflow_version = '1.15'  # Needed internally -- DisableOnExport\u001b[39;00m\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m (distutils\u001b[38;5;241m.\u001b[39mversion\u001b[38;5;241m.\u001b[39mLooseVersion(tf\u001b[38;5;241m.\u001b[39m__version__) \u001b[38;5;241m<\u001b[39m\n\u001b[0;32m     58\u001b[0m       distutils\u001b[38;5;241m.\u001b[39mversion\u001b[38;5;241m.\u001b[39mLooseVersion(required_tensorflow_version)):\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m     60\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThis version of TensorFlow Probability requires TensorFlow \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     61\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mversion >= \u001b[39m\u001b[38;5;132;01m{required}\u001b[39;00m\u001b[38;5;124m; Detected an installation of version \u001b[39m\u001b[38;5;132;01m{present}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     62\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPlease upgrade TensorFlow to proceed.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m     63\u001b[0m             required\u001b[38;5;241m=\u001b[39mrequired_tensorflow_version,\n\u001b[0;32m     64\u001b[0m             present\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39m__version__))\n\u001b[0;32m     66\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m (package \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmcmc\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m     67\u001b[0m       tf\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mtensor_float_32_execution_enabled()):\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# Must import here, because symbols get pruned to __all__.\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: This version of TensorFlow Probability requires TensorFlow version >= 2.18; Detected an installation of version 2.10.0. Please upgrade TensorFlow to proceed."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from qiskit import QuantumCircuit\n",
    "from qiskit_aer.backends.qasm_simulator import QasmSimulator\n",
    "import qiskit.quantum_info as qi\n",
    "from tensorflow_probability.python.distributions import Normal\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Union\n",
    "import csv\n",
    "\n",
    "\"\"\"This code sets the simplest RL algorithm (Policy Gradient) for solving a quantum control problem. The goal is the \n",
    "following: We have access to a quantum computer (here a simulator provided by IBM Q) containing one qubit. The qubit \n",
    "originally starts in state |0>, and we would like to apply a quantum gate (operation) to bring it to state |1>.\n",
    "To do so, we have access to a gate parametrized with an angle, and the RL agent must find the optimal angle \n",
    "maximizing the probability of measuring the |1> state. Optimal value for amplitude amp (angle/2π) is \n",
    "0.5 or -0.5. The RL agent chooses its actions (that is picks a random value for amp) by sampling a number from \n",
    "a Gaussian distribution, of mean mu and standard deviation sigma. The trainable parameters are therefore those two \n",
    "latter variables (we expect the mean to be close to 0.5 and the variance very low). \n",
    "The reward is a binary number obtained upon measurement (only two possible outcomes can be measured). \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def perform_action(\n",
    "    amp: Union[tf.Tensor, np.array], shots=1, target_state=\"|1>\", epoch=1\n",
    "):\n",
    "    \"\"\"\n",
    "    Execute quantum circuit with parametrized amplitude, retrieve measurement result and assign rewards accordingly\n",
    "    :param amp: amplitude parameter, provided as an array of size batchsize\n",
    "    :param shots: number of evaluations to be done on the quantum computer (for simplicity stays to 1)\n",
    "    :param target_state: String indicating which target state is intended (can currently only be \"|1>\" or \"|->\")\n",
    "    :return: Reward table (reward for each run in the batch)\n",
    "    \"\"\"\n",
    "    global qc, qasm, tgt_string  # Use same quantum circuit instance to be reset after each run\n",
    "    angles, batch = np.array(amp), len(np.array(amp))\n",
    "    density_matrix = np.zeros([2, 2], dtype=\"complex128\")\n",
    "    reward_table = np.zeros(batch)\n",
    "\n",
    "    for j, angle in enumerate(angles):\n",
    "        if tgt_string == \"|1>\":\n",
    "            qc.rx(\n",
    "                2 * np.pi * angle, 0\n",
    "            )  # Add parametrized gate for each amplitude in the batch\n",
    "            q_state = qi.Statevector.from_instruction(qc)\n",
    "            density_matrix += (\n",
    "                np.array(q_state.to_operator()) / batch\n",
    "            )  # Build density matrix as a statistical mixture of\n",
    "            # states created by the different actions\n",
    "        elif tgt_string == \"|+>\":\n",
    "            qc.ry(\n",
    "                2 * np.pi * angle, 0\n",
    "            )  # Add parametrized gate for each amplitude in the batch\n",
    "            q_state = qi.Statevector.from_instruction(qc)\n",
    "            density_matrix += (\n",
    "                np.array(q_state.to_operator()) / batch\n",
    "            )  # Build density matrix as a statistical mixture\n",
    "            # of states created by the different actions\n",
    "            qc.h(0)  # Rotate qubit for measurement in Hadamard basis\n",
    "\n",
    "        qc.measure(0, 0)  # Measure the qubit\n",
    "\n",
    "        job = qasm.run(qc, shots=shots)\n",
    "        result = job.result()\n",
    "        counts = result.get_counts(\n",
    "            qc\n",
    "        )  # Returns dictionary with keys '0' and '1' with number of counts for each key\n",
    "\n",
    "        #  Calculate reward (Generalized to include any number of shots for each action)\n",
    "        if tgt_string == \"|1>\":\n",
    "            reward_table[j] += np.mean(\n",
    "                np.array([1] * counts.get(\"1\", 0) + [-1] * counts.get(\"0\", 0))\n",
    "            )\n",
    "        elif tgt_string == \"|+>\":\n",
    "            reward_table[j] += np.mean(\n",
    "                np.array([1] * counts.get(\"0\", 0) + [-1] * counts.get(\"1\", 0))\n",
    "            )\n",
    "        qc.clear()  # Reset the Quantum Circuit for next iteration\n",
    "\n",
    "    return reward_table, qi.DensityMatrix(\n",
    "        density_matrix\n",
    "    )  # reward_table is of Shape [batchsize]\n",
    "\n",
    "\n",
    "# Variables to define environment\n",
    "qc = QuantumCircuit(1, 1, name=\"qc\")  # Two-level system of interest, 1 qubit\n",
    "qasm = QasmSimulator(method=\"statevector\")  # Simulation backend (mock quantum computer)\n",
    "\n",
    "# TODO:: Define a reward function/circuit for each target state in the dictionary\n",
    "target_states_list = {\n",
    "    \"|1>\": qi.DensityMatrix(np.array([[0.0], [1.0]]) @ np.array([[0.0, 1.0]])),\n",
    "    \"|+>\": qi.DensityMatrix(0.5 * np.array([[1.0], [1.0]]) @ np.array([[1.0, 1.0]])),\n",
    "}\n",
    "tgt_string = \"|+>\"\n",
    "\n",
    "# Hyperparameters for the agent\n",
    "seed = 2364  # Seed for action sampling (ref 2763)\n",
    "\n",
    "optimizer_string = \"Adam\"\n",
    "\n",
    "n_epochs = 60\n",
    "batch_size = 50\n",
    "eta = 0.1  # Learning rate for policy update step\n",
    "\n",
    "critic_loss_coeff = 0.5\n",
    "\n",
    "use_PPO = True\n",
    "epsilon = 0.2  # Parameter for ratio clipping value (PPO)\n",
    "grad_clip = 0.3\n",
    "sigma_eps = 1e-6\n",
    "\n",
    "optimizer = None\n",
    "if optimizer_string == \"Adam\":\n",
    "    optimizer = tf.optimizers.Adam(learning_rate=eta)\n",
    "elif optimizer_string == \"SGD\":\n",
    "    optimizer = tf.optimizers.SGD(learning_rate=eta)\n",
    "\n",
    "\n",
    "def constrain_mean_value(mu_var):\n",
    "    return tf.clip_by_value(mu_var, -1.0, 1.0)\n",
    "\n",
    "\n",
    "def constrain_std_value(std_var):\n",
    "    return tf.clip_by_value(std_var, 1e-3, 3)\n",
    "\n",
    "\n",
    "# Policy parameters\n",
    "mu = tf.Variable(\n",
    "    initial_value=tf.random.normal([], stddev=0.05),\n",
    "    trainable=True,\n",
    "    name=\"µ\",\n",
    "    constraint=constrain_mean_value,\n",
    ")\n",
    "sigma = tf.Variable(\n",
    "    initial_value=1.0, trainable=True, name=\"sigma\", constraint=constrain_std_value\n",
    ")\n",
    "\n",
    "# Old parameters are updated with one-step delay, necessary for PPO implementation\n",
    "mu_old = tf.Variable(initial_value=mu, trainable=False, name=\"µ_old\")\n",
    "sigma_old = tf.Variable(initial_value=sigma, trainable=False, name=\"sigma_old\")\n",
    "# Critic parameter (single state-independent baseline b)\n",
    "\n",
    "b = tf.Variable(initial_value=0.0, name=\"baseline\", trainable=True)\n",
    "\n",
    "#  Keep track of variables\n",
    "data = {\n",
    "    \"means\": np.zeros(n_epochs),\n",
    "    \"stds\": np.zeros(n_epochs),\n",
    "    \"amps\": np.zeros([n_epochs, batch_size]),\n",
    "    \"rewards\": np.zeros([n_epochs, batch_size]),\n",
    "    \"critic_loss\": np.zeros(n_epochs),\n",
    "    \"fidelity\": np.zeros(n_epochs),\n",
    "    \"grads\": np.zeros((n_epochs, 3)),\n",
    "    \"hyperparams\": {\n",
    "        \"learning_rate\": eta,\n",
    "        \"seed\": seed,\n",
    "        \"clipping_PPO\": epsilon,\n",
    "        \"grad_clip_value\": grad_clip,\n",
    "        \"n_epochs\": n_epochs,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"target_state\": (tgt_string, target_states_list[tgt_string]),\n",
    "        \"PPO?\": use_PPO,\n",
    "        \"critic_loss_coeff\": critic_loss_coeff,\n",
    "        \"optimizer\": optimizer_string,\n",
    "    },\n",
    "}\n",
    "save_data = False  # Decide if data should be saved in a csv file\n",
    "\n",
    "policy_params = \"Policy params:\"\n",
    "for i in tqdm(range(n_epochs)):\n",
    "    print(\"EPOCH\", i)\n",
    "    print(f\"{policy_params:#<100}\")\n",
    "    print(np.array(mu), \"+-\", np.array(sigma))\n",
    "    print(\"baseline\", np.array(b))\n",
    "\n",
    "    # Sample action from policy (Gaussian distribution with parameters mu and sigma)\n",
    "    Normal_distrib = Normal(\n",
    "        loc=mu, scale=sigma, validate_args=True, allow_nan_stats=False\n",
    "    )\n",
    "    Normal_distrib_old = Normal(\n",
    "        loc=mu_old, scale=sigma_old, validate_args=True, allow_nan_stats=False\n",
    "    )\n",
    "    a = Normal_distrib.sample(batch_size)\n",
    "    # Run quantum circuit to retrieve rewards (in this example, only one time step)\n",
    "    reward, dm_observed = perform_action(a, shots=1, target_state=tgt_string, epoch=i)\n",
    "    print(\"Average Return:\", np.array(tf.reduce_mean(reward)))\n",
    "\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        \"\"\"\n",
    "        Calculate loss function (average return to be maximized, therefore the minus sign placed in front of the loss\n",
    "        since applying gradients minimize the loss), E[R*log(proba(amp)] where proba is the gaussian\n",
    "        probability density (cf paper of reference, educational example).\n",
    "        In case of the PPO, loss function is slightly changed.\n",
    "        \"\"\"\n",
    "\n",
    "        advantage = reward - b\n",
    "        if use_PPO:\n",
    "            ratio = Normal_distrib.prob(a) / (Normal_distrib_old.prob(a) + sigma_eps)\n",
    "            clipped_ratio = tf.clip_by_value(ratio, 1 - epsilon, 1 + epsilon)\n",
    "            actor_loss = -tf.reduce_mean(\n",
    "                tf.minimum(advantage * ratio, advantage * clipped_ratio)\n",
    "            )\n",
    "\n",
    "        else:  # REINFORCE algorithm\n",
    "            log_probs = Normal_distrib.log_prob(a)\n",
    "            actor_loss = -tf.reduce_mean(advantage * log_probs)\n",
    "\n",
    "        critic_loss = tf.reduce_mean(advantage**2)\n",
    "\n",
    "        combined_loss = actor_loss + critic_loss_coeff * critic_loss\n",
    "    # Compute gradients\n",
    "\n",
    "    combined_grads = tape.gradient(combined_loss, tape.watched_variables())\n",
    "    grads = tf.clip_by_value(combined_grads, -grad_clip, grad_clip)\n",
    "\n",
    "    # For PPO, update old parameters to have access to \"old\" policy\n",
    "    if use_PPO:\n",
    "        mu_old.assign(mu)\n",
    "        sigma_old.assign(sigma)\n",
    "\n",
    "    # Apply gradients\n",
    "    optimizer.apply_gradients(zip(grads, tape.watched_variables()))\n",
    "\n",
    "    data[\"amps\"][i] = np.array(a)\n",
    "    data[\"rewards\"][i] = reward\n",
    "    data[\"means\"][i] = np.array(mu)\n",
    "    data[\"stds\"][i] = np.array(sigma)\n",
    "    data[\"critic_loss\"][i] = np.array(critic_loss)\n",
    "    data[\"fidelity\"][i] = qi.state_fidelity(target_states_list[tgt_string], dm_observed)\n",
    "    data[\"grads\"][i] = grads\n",
    "\n",
    "# print(data)\n",
    "\n",
    "if save_data:\n",
    "    w = csv.writer(open(f\"output_seed{seed}_lr{eta}.csv\", \"w\"))\n",
    "\n",
    "    # loop over dictionary keys and values\n",
    "    for key, val in data.items():\n",
    "        # write every key and value to file\n",
    "        w.writerow([key, val])\n",
    "\n",
    "\"\"\"\n",
    "-----------------------------------------------------------------------------------------\n",
    "-----------------------------------------------------------------------------------------\n",
    "-----------------------------------------------------------------------------------------\n",
    "Plotting tools\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "#  Plotting results\n",
    "def plot_examples(ax, reward_table):\n",
    "    \"\"\"\n",
    "    Helper function to plot data with associated colormap, used for plotting the reward per each epoch and each episode\n",
    "    (From original repo associated to the paper https://github.com/v-sivak/quantum-control-rl)\n",
    "    \"\"\"\n",
    "\n",
    "    vals = np.where(reward_table == 1, 0.6, -0.9)\n",
    "\n",
    "    ax.pcolormesh(np.transpose(vals), cmap=\"RdYlGn\", vmin=-1, vmax=1)\n",
    "\n",
    "    ax.set_xticks(np.arange(0, vals.shape[0], 1), minor=True)\n",
    "    ax.set_yticks(np.arange(0, vals.shape[1], 1), minor=True)\n",
    "    ax.grid(which=\"both\", color=\"w\", linestyle=\"-\")\n",
    "    ax.set_aspect(\"equal\")\n",
    "    ax.set_ylabel(\"Episode\")\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "x = np.linspace(-1.0, 1.0, 300)\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3)\n",
    "# Plot probability density associated to updated parameters for a few steps\n",
    "for i in np.linspace(0, n_epochs - 1, 6, dtype=int):\n",
    "    ax1.plot(\n",
    "        x,\n",
    "        norm.pdf(x, loc=data[\"means\"][i], scale=np.abs(data[\"stds\"][i])),\n",
    "        \"-o\",\n",
    "        label=f\"{i}\",\n",
    "    )\n",
    "\n",
    "ax1.set_xlabel(\"Action, a\")\n",
    "ax1.set_ylabel(\"Probability density\")\n",
    "ax1.set_ylim(0.0, 20)\n",
    "#  Plot return as a function of epochs\n",
    "ax2.plot(np.mean(data[\"rewards\"], axis=1), \"-o\", label=\"Return\")\n",
    "ax2.set_xlabel(\"Epoch\")\n",
    "ax2.set_ylabel(\"Expected return\")\n",
    "# ax2.plot(data[\"critic_loss\"], '-.', label='Critic Loss')\n",
    "ax2.plot(\n",
    "    data[\"fidelity\"],\n",
    "    \"-o\",\n",
    "    label=f\"State Fidelity (target: {tgt_string})\",\n",
    "    color=\"green\",\n",
    ")\n",
    "ax2.legend()\n",
    "ax1.legend()\n",
    "plot_examples(ax3, data[\"rewards\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775fe316-ab98-46a9-82a3-11336841e5f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
